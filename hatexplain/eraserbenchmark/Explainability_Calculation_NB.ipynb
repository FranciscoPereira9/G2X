{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will generate the Plausibility and Faithfulness results for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Expainability metrices are based on the work by DeYoung et. al. (2020) [ERASER: A Benchmark to Evaluate Rationalized NLP Models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model choices include 'bert','bert_supervised','birnn','cnngru','birnn_att','birnn_scrat'\n",
    "# attention lambda for bert_supervised is 0.001 and birnn_scrat is 100\n",
    "# for testing_with_lime.py set number of samples is 100 (for faster inference set it lower but the results might not be consistent)\n",
    "\n",
    "#python testing_with_rational.py <model> <att_lambda>\n",
    "!python testing_with_rational.py bert_supervised 0.001\n",
    "#python testing_with_lime.py <model> <number of samples> <att_lambda>\n",
    "!python testing_with_lime.py bert_supervised 100 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>NOTE: Please generate the model explainability output before running this notebook</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import more_itertools as mit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ekphrasis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15664/370669575.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get_annotated_data method is used to load the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPreprocess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataCollect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Work\\UvA_DataScience\\QUVA\\Development\\G2X\\hatexplain\\Preprocess\\dataCollect.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdifflib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequenceMatcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpreProcess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mek_extra_preprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mattentionCal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maggregate_attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mspanMatcher\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreturnMask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturnMaskonetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Work\\UvA_DataScience\\QUVA\\Development\\G2X\\hatexplain\\Preprocess\\preProcess.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mekphrasis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextPreProcessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mekphrasis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSocialTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mekphrasis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdicts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memoticons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0memoticons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#from transformers import BertTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ekphrasis'"
     ]
    }
   ],
   "source": [
    "# get_annotated_data method is used to load the dataset\n",
    "from Preprocess import *\n",
    "from Preprocess.dataCollect import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotated_data(params):\n",
    "    #temp_read = pd.read_pickle(params['data_file'])\n",
    "    with open(params['data_file'], 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    dict_data=[]\n",
    "    for key in data:\n",
    "        temp={}\n",
    "        temp['post_id']=key\n",
    "        temp['text']=data[key]['post_tokens']\n",
    "        final_label=[]\n",
    "        for i in range(1,4):\n",
    "            temp['annotatorid'+str(i)]=data[key]['annotators'][i-1]['annotator_id']\n",
    "#             temp['explain'+str(i)]=data[key]['annotators'][i-1]['rationales']\n",
    "            temp['target'+str(i)]=data[key]['annotators'][i-1]['target']\n",
    "            temp['label'+str(i)]=data[key]['annotators'][i-1]['label']\n",
    "            final_label.append(temp['label'+str(i)])\n",
    "\n",
    "        final_label_id=max(final_label,key=final_label.count)\n",
    "        temp['rationales']=data[key]['rationales']\n",
    "            \n",
    "        if(params['class_names']=='Data/classes_two.npy'):\n",
    "            if(final_label.count(final_label_id)==1):\n",
    "                temp['final_label']='undecided'\n",
    "            else:\n",
    "                if(final_label_id in ['hatespeech','offensive']):\n",
    "                    final_label_id='toxic'\n",
    "                else:\n",
    "                    final_label_id='non-toxic'\n",
    "                temp['final_label']=final_label_id\n",
    "\n",
    "        \n",
    "        else:\n",
    "            if(final_label.count(final_label_id)==1):\n",
    "                temp['final_label']='undecided'\n",
    "            else:\n",
    "                temp['final_label']=final_label_id\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        dict_data.append(temp)    \n",
    "    temp_read = pd.DataFrame(dict_data)  \n",
    "    return temp_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data_folder={\n",
    "      '2':{'data_file':'data/dataset.json','class_label':'data/classes_two.npy'},\n",
    "      '3':{'data_file':'data/dataset.json','class_label':'data/classes.npy'}\n",
    "}\n",
    "\n",
    "# We need to load the dataset with the labels as 'hatespeech', 'offensive', and 'normal' (3-class). \n",
    "params = {}\n",
    "params['num_classes']=3\n",
    "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "data_all_labelled=get_annotated_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Normal tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# The important key here is the 'bert_token'. Set it to True for Bert based models and False for Others.\n",
    "\n",
    "params_data={\n",
    "    'include_special':False,  #True is want to include <url> in place of urls if False will be removed\n",
    "    'bert_tokens':False, #True /False\n",
    "    'type_attention':'softmax', #softmax\n",
    "    'set_decay':0.1,\n",
    "    'majority':2,\n",
    "    'max_length':128,\n",
    "    'variance':10,\n",
    "    'window':4,\n",
    "    'alpha':0.5,\n",
    "    'p_value':0.8,\n",
    "    'method':'additive',\n",
    "    'decay':False,\n",
    "    'normalized':False,\n",
    "    'not_recollect':True,\n",
    "}\n",
    "\n",
    "\n",
    "if(params_data['bert_tokens']):\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
    "else:\n",
    "    print('Loading Normal tokenizer...')\n",
    "    tokenizer=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnMask(row,params,tokenizer):\n",
    "    \n",
    "    text_tokens=row['text']\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### a very rare corner case\n",
    "    if(len(text_tokens)==0):\n",
    "        text_tokens=['dummy']\n",
    "        print(\"length of text ==0\")\n",
    "    #####\n",
    "    \n",
    "    \n",
    "    mask_all= row['rationales']\n",
    "    mask_all_temp=mask_all\n",
    "    count_temp=0\n",
    "    while(len(mask_all_temp)!=3):\n",
    "        mask_all_temp.append([0]*len(text_tokens))\n",
    "    \n",
    "    word_mask_all=[]\n",
    "    word_tokens_all=[]\n",
    "    \n",
    "    for mask in mask_all_temp:\n",
    "        if(mask[0]==-1):\n",
    "            mask=[0]*len(mask)\n",
    "        \n",
    "        \n",
    "        list_pos=[]\n",
    "        mask_pos=[]\n",
    "        \n",
    "        flag=0\n",
    "        for i in range(0,len(mask)):\n",
    "            if(i==0 and mask[i]==0):\n",
    "                list_pos.append(0)\n",
    "                mask_pos.append(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if(flag==0 and mask[i]==1):\n",
    "                mask_pos.append(1)\n",
    "                list_pos.append(i)\n",
    "                flag=1\n",
    "                \n",
    "            elif(flag==1 and mask[i]==0):\n",
    "                flag=0\n",
    "                mask_pos.append(0)\n",
    "                list_pos.append(i)\n",
    "        if(list_pos[-1]!=len(mask)):\n",
    "            list_pos.append(len(mask))\n",
    "            mask_pos.append(0)\n",
    "        string_parts=[]\n",
    "        for i in range(len(list_pos)-1):\n",
    "            string_parts.append(text_tokens[list_pos[i]:list_pos[i+1]])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if(params['bert_tokens']):\n",
    "            word_tokens=[101]\n",
    "            word_mask=[0]\n",
    "        else:\n",
    "            word_tokens=[]\n",
    "            word_mask=[]\n",
    "\n",
    "        \n",
    "        for i in range(0,len(string_parts)):\n",
    "            tokens=ek_extra_preprocess(\" \".join(string_parts[i]),params,tokenizer)\n",
    "            masks=[mask_pos[i]]*len(tokens)\n",
    "            word_tokens+=tokens\n",
    "            word_mask+=masks\n",
    "\n",
    "\n",
    "        if(params['bert_tokens']):\n",
    "            ### always post truncation\n",
    "            word_tokens=word_tokens[0:(int(params['max_length'])-2)]\n",
    "            word_mask=word_mask[0:(int(params['max_length'])-2)]\n",
    "            word_tokens.append(102)\n",
    "            word_mask.append(0)\n",
    "\n",
    "        word_mask_all.append(word_mask)\n",
    "        word_tokens_all.append(word_tokens)\n",
    "        \n",
    "#     for k in range(0,len(mask_all)):\n",
    "#          if(mask_all[k][0]==-1):\n",
    "#             word_mask_all[k] = [-1]*len(word_mask_all[k])\n",
    "    if(len(mask_all)==0):\n",
    "        word_mask_all=[]\n",
    "    else:    \n",
    "        word_mask_all=word_mask_all[0:len(mask_all)]\n",
    "    return word_tokens_all[0],word_mask_all    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ek_extra_preprocess(text,params,tokenizer):\n",
    "    remove_words=['<allcaps>','</allcaps>','<hashtag>','</hashtag>','<elongated>','<emphasis>','<repeated>','\\'','s']\n",
    "    word_list=text_processor.pre_process_doc(text)\n",
    "    if(params['include_special']):\n",
    "        pass\n",
    "    else:\n",
    "        word_list=list(filter(lambda a: a not in remove_words, word_list)) \n",
    "    if(params['bert_tokens']):\n",
    "        sent=\" \".join(word_list)\n",
    "        sent = re.sub(r\"[<\\*>]\", \" \",sent)\n",
    "        sub_word_list = custom_tokenize(sent,tokenizer)\n",
    "        return sub_word_list\n",
    "    else:            \n",
    "        word_list=[token for token in word_list if token not in string.punctuation]\n",
    "        return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the whole dataset and get the tokenwise rationales\n",
    "def get_training_data(data):\n",
    "    post_ids_list=[]\n",
    "    text_list=[]\n",
    "    attention_list=[]\n",
    "    label_list=[]\n",
    "    \n",
    "    final_binny_output = []\n",
    "    print('total_data',len(data))\n",
    "    for index,row in tqdm(data.iterrows(),total=len(data)):\n",
    "        annotation=row['final_label']\n",
    "        \n",
    "        text=row['text']\n",
    "        post_id=row['post_id']\n",
    "        annotation_list=[row['label1'],row['label2'],row['label3']]\n",
    "        tokens_all = list(row['text'])\n",
    "#         attention_masks =  [list(row['explain1']),list(row['explain2']),list(row['explain1'])]\n",
    "        \n",
    "        if(annotation!= 'undecided'):\n",
    "            tokens_all,attention_masks=returnMask(row, params_data, tokenizer)\n",
    "            final_binny_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])\n",
    "\n",
    "    return final_binny_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_data 20148\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbff4b4c91a241199f9ce2616e2b36fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20148.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15664/3354933341.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_all_labelled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15664/4263594293.py\u001b[0m in \u001b[0;36mget_training_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[1;33m!=\u001b[0m \u001b[1;34m'undecided'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mtokens_all\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturnMask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mfinal_binny_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpost_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotation_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15664/1603495712.py\u001b[0m in \u001b[0;36mreturnMask\u001b[1;34m(row, params, tokenizer)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring_parts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mtokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mek_extra_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring_parts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mmasks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mword_tokens\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15664/354817487.py\u001b[0m in \u001b[0;36mek_extra_preprocess\u001b[1;34m(text, params, tokenizer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mek_extra_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mremove_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<allcaps>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'</allcaps>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'<hashtag>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'</hashtag>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'<elongated>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'<emphasis>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'<repeated>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mword_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_process_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'include_special'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_processor' is not defined"
     ]
    }
   ],
   "source": [
    "training_data=get_training_data(data_all_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18696652_gab',\n",
       " 'offensive',\n",
       " [101,\n",
       "  5310,\n",
       "  25805,\n",
       "  5582,\n",
       "  4319,\n",
       "  2224,\n",
       "  2025,\n",
       "  11382,\n",
       "  3489,\n",
       "  2012,\n",
       "  2035,\n",
       "  4283,\n",
       "  2005,\n",
       "  2008,\n",
       "  19380,\n",
       "  102],\n",
       " [[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
       " ['hatespeech', 'offensive', 'offensive']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list\n",
    "def find_ranges(iterable):\n",
    "    \"\"\"Yield range of consecutive numbers.\"\"\"\n",
    "    for group in mit.consecutive_groups(iterable):\n",
    "        group = list(group)\n",
    "        if len(group) == 1:\n",
    "            yield group[0]\n",
    "        else:\n",
    "            yield group[0], group[-1]\n",
    "            \n",
    "# Convert dataset into ERASER format: https://github.com/jayded/eraserbenchmark/blob/master/rationale_benchmark/utils.py\n",
    "def get_evidence(post_id, anno_text, explanations):\n",
    "    output = []\n",
    "\n",
    "    indexes = sorted([i for i, each in enumerate(explanations) if each==1])\n",
    "    span_list = list(find_ranges(indexes))\n",
    "\n",
    "    for each in span_list:\n",
    "        if type(each)== int:\n",
    "            start = each\n",
    "            end = each+1\n",
    "        elif len(each) == 2:\n",
    "            start = each[0]\n",
    "            end = each[1]+1\n",
    "        else:\n",
    "            print('error')\n",
    "\n",
    "        output.append({\"docid\":post_id, \n",
    "              \"end_sentence\": -1, \n",
    "              \"end_token\": end, \n",
    "              \"start_sentence\": -1, \n",
    "              \"start_token\": start, \n",
    "              \"text\": ' '.join([str(x) for x in anno_text[start:end]])})\n",
    "    return output\n",
    "\n",
    "# To use the metrices defined in ERASER, we will have to convert the dataset\n",
    "def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):  \n",
    "    final_output = []\n",
    "    \n",
    "    if save_split:\n",
    "        train_fp = open(save_path+'train.jsonl', 'w')\n",
    "        val_fp = open(save_path+'val.jsonl', 'w')\n",
    "        test_fp = open(save_path+'test.jsonl', 'w')\n",
    "            \n",
    "    for tcount, eachrow in enumerate(dataset):\n",
    "        \n",
    "        temp = {}\n",
    "        post_id = eachrow[0]\n",
    "        post_class = eachrow[1]\n",
    "        anno_text_list = eachrow[2]\n",
    "        majority_label = eachrow[1]\n",
    "        \n",
    "        if majority_label=='normal':\n",
    "            continue\n",
    "        \n",
    "        all_labels = eachrow[4]\n",
    "        explanations = []\n",
    "        for each_explain in eachrow[3]:\n",
    "            explanations.append(list(each_explain))\n",
    "        \n",
    "        # For this work, we have considered the union of explanations. Other options could be explored as well.\n",
    "        if method == 'union':\n",
    "            final_explanation = [any(each) for each in zip(*explanations)]\n",
    "            final_explanation = [int(each) for each in final_explanation]\n",
    "        \n",
    "            \n",
    "        temp['annotation_id'] = post_id\n",
    "        temp['classification'] = post_class\n",
    "        temp['evidences'] = [get_evidence(post_id, list(anno_text_list), final_explanation)]\n",
    "        temp['query'] = \"What is the class?\"\n",
    "        temp['query_type'] = None\n",
    "        final_output.append(temp)\n",
    "        \n",
    "        if save_split:\n",
    "            if not os.path.exists(save_path+'docs'):\n",
    "                os.makedirs(save_path+'docs')\n",
    "            \n",
    "            with open(save_path+'docs/'+post_id, 'w') as fp:\n",
    "                fp.write(' '.join([str(x) for x in list(anno_text_list)]))\n",
    "            \n",
    "            if post_id in id_division['train']:\n",
    "                train_fp.write(json.dumps(temp)+'\\n')\n",
    "            \n",
    "            elif post_id in id_division['val']:\n",
    "                val_fp.write(json.dumps(temp)+'\\n')\n",
    "            \n",
    "            elif post_id in id_division['test']:\n",
    "                test_fp.write(json.dumps(temp)+'\\n')\n",
    "            else:\n",
    "                print(post_id)\n",
    "    \n",
    "    if save_split:\n",
    "        train_fp.close()\n",
    "        val_fp.close()\n",
    "        test_fp.close()\n",
    "        \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
    "with open('./data/post_id_divisions.json') as fp:\n",
    "    id_division = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14292/3389041196.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msave_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./Data/Evaluation/Model_Eval/'\u001b[0m  \u001b[1;31m#The dataset in Eraser Format will be stored here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mconvert_to_eraser_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_division\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "method = 'union'\n",
    "save_split = True\n",
    "save_path = './Data/Evaluation/Model_Eval/'  #The dataset in Eraser Format will be stored here.\n",
    "convert_to_eraser_format(training_data, method, save_split, save_path, id_division)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd eraserbenchmark-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"rationale_benchmark/metrics.py\", line 670, in <module>\n",
      "    main()\n",
      "  File \"rationale_benchmark/metrics.py\", line 610, in main\n",
      "    docs = load_flattened_documents(args.data_dir, docids)\n",
      "  File \"C:\\Users\\kicop\\OneDrive\\Work\\UvA_DataScience\\QUVA\\Development\\G2X\\hatexplain\\eraserbenchmark\\rationale_benchmark\\utils.py\", line 165, in load_flattened_documents\n",
      "    unflattened_docs = load_documents(data_dir, docids)\n",
      "  File \"C:\\Users\\kicop\\OneDrive\\Work\\UvA_DataScience\\QUVA\\Development\\G2X\\hatexplain\\eraserbenchmark\\rationale_benchmark\\utils.py\", line 152, in load_documents\n",
      "    with open(os.path.join(docs_dir, d), 'r') as inf:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data\\\\docs\\\\10068208_gab'\n"
     ]
    }
   ],
   "source": [
    "#--data_dir : Location of the folder which contains the dataset in eraser format\n",
    "#--results : The location of the model output file in eraser format\n",
    "#--score_file : The file name and location to write the output\n",
    "\n",
    "!python rationale_benchmark/metrics.py --split test --strict --data_dir ../data --results ../data/output.jsonl --score_file ../data/model_explain_output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the required results\n",
    "with open('../model_explain_output.json') as fp:\n",
    "    output_data = json.load(fp)\n",
    "\n",
    "print('\\nPlausibility')\n",
    "print('IOU F1 :', output_data['iou_scores'][0]['macro']['f1'])\n",
    "print('Token F1 :', output_data['token_prf']['instance_macro']['f1'])\n",
    "print('AUPRC :', output_data['token_soft_metrics']['auprc'])\n",
    "\n",
    "print('\\nFaithfulness')\n",
    "print('Comprehensiveness :', output_data['classification_scores']['comprehensiveness'])\n",
    "print('Sufficiency', output_data['classification_scores']['sufficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
